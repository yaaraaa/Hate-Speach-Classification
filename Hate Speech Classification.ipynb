{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySsIPBRiorxK"
   },
   "source": [
    "**Dataset**\n",
    "labeled dataset collected from twitter (Hate Speech.tsv)\n",
    "\n",
    "**Objective**\n",
    "classify tweets containing hate speech from other tweets. <br>\n",
    "0 -> no hate speech <br>\n",
    "1 -> contains hate speech <br>\n",
    "\n",
    "**Evaluation metric**\n",
    "macro f1 score\n",
    "\n",
    "**Steps**\n",
    "\n",
    "To classify hate speech in tweets, follow these key steps:\n",
    "\n",
    "1. **Data Preprocessing**: Clean text (remove punctuation, stopwords, etc.), lowercase, tokenize, and so on.\n",
    "2. **Text Representation**: Use Bag of Words, TF-IDF, or word embeddings (e.g., GloVe, Word2Vec, or FastText).\n",
    "3. **Modeling Approaches**:\n",
    "   - **Traditional Models**: Logistic Regression, Naive Bayes, SVM, Random Forest.\n",
    "   - **Deep Learning**: LSTM or RNN.\n",
    "4. **Evaluation**\n",
    "5. **Optimization**: Use hyperparameter tuning, regularization, and ensemble methods for better performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-6lrKz6orxT"
   },
   "source": [
    "### Import used libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "eXUPo3g4orxV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fG8MkuvjorxX"
   },
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note: search how to load the data from tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BYeqhp66orxY"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label  \\\n",
       "0   1      0   \n",
       "1   2      0   \n",
       "2   3      0   \n",
       "3   4      0   \n",
       "4   5      0   \n",
       "\n",
       "                                                                                                                        tweet  \n",
       "0                       @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run  \n",
       "1  @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked  \n",
       "2                                                                                                         bihday your majesty  \n",
       "3                                        #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦  \n",
       "4                                                                                      factsguide: society now    #motivation  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Hate Speech.tsv\", sep= \"\\t\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31535"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good practice to split the data before EDA helps maintain the integrity of the machine learning process, prevents data leakage, simulates real-world scenarios more accurately, and ensures reliable model performance evaluation on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22074 4730 4731\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, rest_df = train_test_split(df, test_size=0.3)\n",
    "val_df, test_df = train_test_split(rest_df, test_size=0.5)\n",
    "\n",
    "print(len(train_df), len(val_df), len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqWVKi_GorxZ"
   },
   "source": [
    "### EDA on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1zxJpFxorxa"
   },
   "source": [
    "- check NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HVEttSujorxa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id       0\n",
      "label    0\n",
      "tweet    0\n",
      "dtype: int64\n",
      "id       0\n",
      "label    0\n",
      "tweet    0\n",
      "dtype: int64\n",
      "id       0\n",
      "label    0\n",
      "tweet    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"{train_df.isna().sum()}\\n{val_df.isna().sum()}\\n{test_df.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwjbzVaIorxb"
   },
   "source": [
    "- check duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "J_FlBWISorxb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjIBFc35orxc"
   },
   "source": [
    "- show a representative sample of data texts to find out required preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zGFKzSCRorxc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yara.mahfouz\\AppData\\Local\\Temp\\ipykernel_3688\\204958866.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampled_data = df.groupby('label').apply(lambda x: x.sample(min(10, len(x)))).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23887</td>\n",
       "      <td>0</td>\n",
       "      <td>@user the dog kind of way to say: âdon't worry, be  !â #dogsarejoy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26141</td>\n",
       "      <td>0</td>\n",
       "      <td>my little bro may have just beat me in basketball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4522</td>\n",
       "      <td>0</td>\n",
       "      <td>good for you || ms     #grunge #nature #rad #awsome #sun #photo #iphone #nofilter #eahâ¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25043</td>\n",
       "      <td>0</td>\n",
       "      <td>@user kids loved the drums. mom couldn`t close the deal with salesmen    #bengaluru street market #wednesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21251</td>\n",
       "      <td>0</td>\n",
       "      <td>happy feet. #throwback #holiday #solotrip #solooverseastrip   #australia #ausboundâ¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8541</td>\n",
       "      <td>0</td>\n",
       "      <td>the fun pa #sarcasm #moving   #thetaylorway #chaos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>898</td>\n",
       "      <td>0</td>\n",
       "      <td>python27 and concurrency are not best friends,   all the code i have 2 restructure to get concurrency with celery #developers #python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>23400</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user @user i had to #factcheck this cuz i was like #wtf can't b true but guess #cnn #media like #meth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4192</td>\n",
       "      <td>0</td>\n",
       "      <td>ah w'd rather be happy than  , cuss it all t' tarnation. varejao more &amp;gt;&amp;gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>31524</td>\n",
       "      <td>0</td>\n",
       "      <td>#life #love be   #enjoy #appreciate #des'tee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>14817</td>\n",
       "      <td>1</td>\n",
       "      <td>#koreans &amp;amp; joseon people in japan, will abuse the  for claims of own rights by rough demo.  #aberdeen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1597</td>\n",
       "      <td>1</td>\n",
       "      <td>@user #allahsoil the problem is that thereâs no space for alternative explanations.  â¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>22321</td>\n",
       "      <td>1</td>\n",
       "      <td>the end of   #me #selfie # #love #messi #cr7 #religion #christianity #mecca_live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12718</td>\n",
       "      <td>1</td>\n",
       "      <td>#paladino ... what can i label him as ..that hasn't been said before? ... just #shameful for a #public servant to hâ¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>30755</td>\n",
       "      <td>1</td>\n",
       "      <td>like msian opposition and their claim that bangladeshis voted for bn. no evidence yet people believe it anywayâ¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6354</td>\n",
       "      <td>1</td>\n",
       "      <td>don't let the #bigot skate on his true record of !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9883</td>\n",
       "      <td>1</td>\n",
       "      <td>great to see quebec is establishing a provincial inquiry into systemic  &amp;amp;  towards indigenous ppls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20777</td>\n",
       "      <td>1</td>\n",
       "      <td>why is 4 whores on ring of fire ?  #misogyny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>13640</td>\n",
       "      <td>1</td>\n",
       "      <td>white girl has sex with black guy sexest women nude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>12701</td>\n",
       "      <td>1</td>\n",
       "      <td>marco rubio has no consideration close to now.  #rwnj #weirdo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  label  \\\n",
       "0   23887      0   \n",
       "1   26141      0   \n",
       "2    4522      0   \n",
       "3   25043      0   \n",
       "4   21251      0   \n",
       "5    8541      0   \n",
       "6     898      0   \n",
       "7   23400      0   \n",
       "8    4192      0   \n",
       "9   31524      0   \n",
       "10  14817      1   \n",
       "11   1597      1   \n",
       "12  22321      1   \n",
       "13  12718      1   \n",
       "14  30755      1   \n",
       "15   6354      1   \n",
       "16   9883      1   \n",
       "17  20777      1   \n",
       "18  13640      1   \n",
       "19  12701      1   \n",
       "\n",
       "                                                                                                                                    tweet  \n",
       "0                                                                 @user the dog kind of way to say: âdon't worry, be  !â #dogsarejoy   \n",
       "1                                                                                       my little bro may have just beat me in basketball  \n",
       "2                                               good for you || ms     #grunge #nature #rad #awsome #sun #photo #iphone #nofilter #eahâ¦  \n",
       "3                            @user kids loved the drums. mom couldn`t close the deal with salesmen    #bengaluru street market #wednesday  \n",
       "4                                                   happy feet. #throwback #holiday #solotrip #solooverseastrip   #australia #ausboundâ¦  \n",
       "5                                                                                      the fun pa #sarcasm #moving   #thetaylorway #chaos  \n",
       "6   python27 and concurrency are not best friends,   all the code i have 2 restructure to get concurrency with celery #developers #python  \n",
       "7                            @user @user @user i had to #factcheck this cuz i was like #wtf can't b true but guess #cnn #media like #meth  \n",
       "8                                                         ah w'd rather be happy than  , cuss it all t' tarnation. varejao more &gt;&gt;   \n",
       "9                                                                                            #life #love be   #enjoy #appreciate #des'tee  \n",
       "10                              #koreans &amp; joseon people in japan, will abuse the  for claims of own rights by rough demo.  #aberdeen  \n",
       "11                                             @user #allahsoil the problem is that thereâs no space for alternative explanations.  â¦  \n",
       "12                                                       the end of   #me #selfie # #love #messi #cr7 #religion #christianity #mecca_live  \n",
       "13                 #paladino ... what can i label him as ..that hasn't been said before? ... just #shameful for a #public servant to hâ¦  \n",
       "14                      like msian opposition and their claim that bangladeshis voted for bn. no evidence yet people believe it anywayâ¦  \n",
       "15                                                                                     don't let the #bigot skate on his true record of !  \n",
       "16                                 great to see quebec is establishing a provincial inquiry into systemic  &amp;  towards indigenous ppls  \n",
       "17                                                                                           why is 4 whores on ring of fire ?  #misogyny  \n",
       "18                                                                                    white girl has sex with black guy sexest women nude  \n",
       "19                                                                          marco rubio has no consideration close to now.  #rwnj #weirdo  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data = df.groupby('label').apply(lambda x: x.sample(min(10, len(x)))).reset_index(drop=True)\n",
    "\n",
    "sampled_data[['id', 'label', 'tweet']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqdSUtbdorxd"
   },
   "source": [
    "- check dataset balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JBHrSvXhorxd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal speech:  92.98240050737276\n",
      "Hate speech:  7.01759949262724\n"
     ]
    }
   ],
   "source": [
    "print(\"Normal speech: \", df.label.value_counts()[0]/len(df)*100)\n",
    "print(\"Hate speech: \", df.label.value_counts()[1]/len(df)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3wl_crkorxd"
   },
   "source": [
    "- Cleaning and Preprocessing are:\n",
    "    - Lowercasing\n",
    "    - Remove user mentions\n",
    "    - Remove URLs\n",
    "    - Remove special characters and punctuation\n",
    "    - Tokenize text\n",
    "    - Remove stopwords\n",
    "    - Handling emojis\n",
    "    - Lemmatization\n",
    "    - Handle abbreviations and slang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyJkqK9gorxe"
   },
   "source": [
    "### Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yara.mahfouz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yara.mahfouz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "slang_dict = {\n",
    "    \"u\": \"you\",\n",
    "    \"ur\": \"your\",\n",
    "    \"r\": \"are\",\n",
    "    \"y\": \"why\",\n",
    "    \"pls\": \"please\",\n",
    "    \"plz\": \"please\",\n",
    "    \"thx\": \"thanks\",\n",
    "    \"ty\": \"thank you\",\n",
    "    \"dm\": \"direct message\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"idk\": \"I don't know\",\n",
    "    \"ikr\": \"I know right\",\n",
    "    \"smh\": \"shaking my head\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"imho\": \"in my humble opinion\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"lol\": \"laughing out loud\",\n",
    "    \"lmao\": \"laughing my ass off\",\n",
    "    \"rofl\": \"rolling on the floor laughing\",\n",
    "    \"wtf\": \"what the fuck\",\n",
    "    \"wth\": \"what the heck\",\n",
    "    \"afaik\": \"as far as I know\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"np\": \"no problem\",\n",
    "    \"bc\": \"because\",\n",
    "    \"b/c\": \"because\",\n",
    "    \"b4\": \"before\",\n",
    "    \"cuz\": \"because\",\n",
    "    \"gf\": \"girlfriend\",\n",
    "    \"bf\": \"boyfriend\",\n",
    "    \"bff\": \"best friends forever\",\n",
    "    \"gr8\": \"great\",\n",
    "    \"l8r\": \"later\",\n",
    "    \"tho\": \"though\",\n",
    "    \"thru\": \"through\",\n",
    "    \"msg\": \"message\",\n",
    "    \"txt\": \"text\",\n",
    "    \"omw\": \"on my way\",\n",
    "    \"fml\": \"fuck my life\",\n",
    "    \"nvm\": \"never mind\",\n",
    "    \"bday\": \"birthday\",\n",
    "    \"tbt\": \"throwback Thursday\",\n",
    "    \"icymi\": \"in case you missed it\",\n",
    "    \"irl\": \"in real life\",\n",
    "    \"ppl\": \"people\",\n",
    "    \"bffl\": \"best friends for life\",\n",
    "    \"jk\": \"just kidding\",\n",
    "    \"xoxo\": \"hugs and kisses\",\n",
    "    \"idc\": \"I don't care\",\n",
    "    \"ily\": \"I love you\",\n",
    "    \"ilu\": \"I love you\",\n",
    "    \"omfg\": \"oh my fucking god\",\n",
    "    \"srsly\": \"seriously\",\n",
    "    \"ikr\": \"I know right\",\n",
    "    \"fam\": \"family\",\n",
    "    \"bae\": \"before anyone else\",\n",
    "    \"hmu\": \"hit me up\",\n",
    "    \"gg\": \"good game\",\n",
    "    \"tmi\": \"too much information\",\n",
    "    \"ftw\": \"for the win\",\n",
    "    \"lit\": \"exciting or fun\",\n",
    "    \"s/o\": \"shoutout\",\n",
    "    \"irl\": \"in real life\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"hbd\": \"happy birthday\",\n",
    "    \"atm\": \"at the moment\",\n",
    "    \"qotd\": \"quote of the day\",\n",
    "    \"rn\": \"right now\",\n",
    "    \"tfw\": \"that feeling when\",\n",
    "    \"yolo\": \"you only live once\",\n",
    "    \"wyd\": \"what are you doing\",\n",
    "    \"wya\": \"where you at\",\n",
    "    \"bb\": \"baby\",\n",
    "    \"luv\": \"love\",\n",
    "    \"gr8\": \"great\",\n",
    "    \"m8\": \"mate\",\n",
    "    \"obvi\": \"obviously\",\n",
    "    \"def\": \"definitely\",\n",
    "    \"jk\": \"just kidding\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Use custom scikit-learn Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using custom transformers in scikit-learn provides flexibility, reusability, and control over the data transformation process, allowing you to seamlessly integrate with scikit-learn's pipelines, enabling you to combine multiple preprocessing steps and modeling into a single workflow. This makes your code more modular, readable, and easier to maintain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### link: https://www.andrewvillazon.com/custom-scikit-learn-transformers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CustomTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Define a function to handle the text preprocessing\n",
    "        def preprocess_tweet(text):\n",
    "            # Lowercasing\n",
    "            text = text.lower()\n",
    "            \n",
    "            # Remove Mentions\n",
    "            text = re.sub(r'@\\w+', '', text)\n",
    "            \n",
    "            # Remove URLs\n",
    "            text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "                        \n",
    "            # Remove Special Characters and Punctuation\n",
    "            text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "            \n",
    "            # Tokenize Text\n",
    "            words = text.split()\n",
    "            \n",
    "            # Remove Stop Words\n",
    "            words = [word for word in words if word not in self.stop_words]\n",
    "            \n",
    "            # Handle Emojis (convert to text)\n",
    "            text = emoji.demojize(\" \".join(words))\n",
    "            \n",
    "            # Lemmatization\n",
    "            words = [self.lemmatizer.lemmatize(word) for word in words]\n",
    "            \n",
    "            # Replace Slang\n",
    "            words = [slang_dict[word] if word in slang_dict else word for word in words]\n",
    "            \n",
    "            # Reconstruct the processed tweet\n",
    "            return \" \".join(words)\n",
    "        \n",
    "        # Apply the preprocessing to each tweet in the 'tweet' column\n",
    "        X = X.copy()\n",
    "        \n",
    "        return X.apply(preprocess_tweet)\n",
    "\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba0r1ASHorxf"
   },
   "source": [
    "**You  are doing Great so far!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9BhRQbYorxf"
   },
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Au1FYDPzorxg"
   },
   "source": [
    "#### Extra: use scikit-learn pipline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### link: https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pipelines in scikit-learn promotes better code organization, reproducibility, and efficiency in machine learning workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LP5FZzmborxg"
   },
   "source": [
    "#### Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9439746300211417\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessing', CustomTransformer()),\n",
    "    ('Vectorizing', TfidfVectorizer()),\n",
    "    ('model', model),\n",
    "])\n",
    "\n",
    "# Split your data into inputs and labels\n",
    "X_train = train_df['tweet']  # Raw tweet text as input\n",
    "y_train = train_df['label']  # Labels for classification\n",
    "\n",
    "X_val = val_df['tweet']\n",
    "y_val = val_df['label']\n",
    "\n",
    "# Fit the pipeline on training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "val_predictions = pipeline.predict(X_val)\n",
    "\n",
    "val_accuracy = pipeline.score(X_val, y_val)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9492707672796449\n"
     ]
    }
   ],
   "source": [
    "X_test = test_df['tweet']  \n",
    "y_test = test_df['label']  \n",
    "\n",
    "test_predictions = pipeline.predict(X_test)\n",
    "\n",
    "test_accuracy = pipeline.score(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85JlkIQXorxg"
   },
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation metric:**\n",
    "macro f1 score\n",
    "\n",
    "Macro F1 score is a useful metric in scenarios where you want to evaluate the overall performance of a multi-class classification model, **particularly when the classes are imbalanced**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Calculation](https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/639c3d934e82c1195cdf3c60_macro-f1.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Macro F1 Score: 0.7211947393751289\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "test_predictions = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate the macro F1 score\n",
    "test_macro_f1 = f1_score(y_test, test_predictions, average='macro')\n",
    "print(\"Validation Macro F1 Score:\", test_macro_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhVFUaIcorxh"
   },
   "source": [
    "### Enhancement\n",
    "\n",
    "- Using different text representation or modeling techniques\n",
    "- Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The basic implementation above produced a very low F1 score despite having good accuracy becasue of class imbalance, i will try to fix it by adjusting class weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Y4h1Danvorxh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'model__C': 10, 'model__penalty': 'l2'}\n",
      "Test Macro F1 Score with best parameters: 0.8498494058768788\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "model = LogisticRegression(class_weight='balanced')\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessing', CustomTransformer()),\n",
    "    ('vectorizing', TfidfVectorizer()),\n",
    "    ('model', model),\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'model__C': [0.01, 0.1, 1, 10],\n",
    "    'model__penalty': ['l2'],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, scoring='f1_macro', cv=5, n_jobs=1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "test_predictions = best_pipeline.predict(X_test)\n",
    "test_macro_f1 = f1_score(y_test, test_predictions, average='macro')\n",
    "print(\"Test Macro F1 Score with best parameters:\", test_macro_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The results are much better, but there is still room for improvement. Let's try a different model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Macro F1 Score with SVM: 0.8398827987241715\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "svm_model = SVC(kernel='rbf', class_weight='balanced')\n",
    "\n",
    "# Define the pipeline with TfidfVectorizer and SVM\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessing', CustomTransformer()),\n",
    "    ('vectorizing', TfidfVectorizer()),\n",
    "    ('model', svm_model)\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "test_predictions = pipeline.predict(X_test)\n",
    "\n",
    "test_macro_f1 = f1_score(y_test, test_predictions, average='macro')\n",
    "print(\"Test Macro F1 Score with SVM:\", test_macro_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Not better, lets try deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_transformer = CustomTransformer()\n",
    "\n",
    "# Transform the text data\n",
    "X_train_cleaned = custom_transformer.fit_transform(X_train)\n",
    "X_val_cleaned = custom_transformer.transform(X_val)\n",
    "X_test_cleaned = custom_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 1293, Count: 1\n",
      "Length: 583, Count: 1\n",
      "Length: 364, Count: 1\n",
      "Length: 193, Count: 1\n",
      "Length: 162, Count: 1\n",
      "Length: 157, Count: 1\n",
      "Length: 130, Count: 1\n",
      "Length: 122, Count: 1\n",
      "Length: 107, Count: 1\n",
      "Length: 82, Count: 1\n",
      "Length: 66, Count: 1\n",
      "Length: 54, Count: 1\n",
      "Length: 51, Count: 1\n",
      "Length: 46, Count: 1\n",
      "Length: 23, Count: 1\n",
      "Length: 21, Count: 3\n",
      "Length: 20, Count: 15\n",
      "Length: 19, Count: 19\n",
      "Length: 18, Count: 20\n",
      "Length: 17, Count: 86\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "MAX_NUM_WORDS = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train_cleaned)\n",
    "\n",
    "# Convert texts to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_cleaned)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val_cleaned)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test_cleaned)\n",
    "\n",
    "all_sequences = X_train_seq + X_val_seq + X_test_seq\n",
    "\n",
    "sequence_lengths = [len(seq) for seq in all_sequences]\n",
    "\n",
    "# Get the counts of each sequence length\n",
    "length_counts = Counter(sequence_lengths)\n",
    "\n",
    "# Sort the lengths in descending order and get the top k values with their counts\n",
    "k = 20\n",
    "top_k_lengths = sorted(length_counts.items(), key=lambda x: x[0], reverse=True)[:k]\n",
    "\n",
    "for length, count in top_k_lengths:\n",
    "    print(f\"Length: {length}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i will drop the top 15 sequences lengths to reduce computation complexity resulting from large padding, and 15 samples is not a large number to drop yet it will provide strong gains in terms of computational complexity. The length od the max sequence will then be set to 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the lengths of each sequence\n",
    "sequence_lengths = [len(seq) for seq in X_train_seq + X_val_seq + X_test_seq]\n",
    "\n",
    "# Identify the top 15 longest lengths\n",
    "from collections import Counter\n",
    "length_counts = Counter(sequence_lengths)\n",
    "top_15_lengths = sorted(length_counts.keys(), reverse=True)[:15]\n",
    "\n",
    "# Define a function to filter sequences and their labels\n",
    "def filter_sequences_and_labels(sequences, labels, lengths_to_remove):\n",
    "    filtered_sequences = []\n",
    "    filtered_labels = []\n",
    "    for seq, label in zip(sequences, labels):\n",
    "        if len(seq) not in lengths_to_remove:\n",
    "            filtered_sequences.append(seq)\n",
    "            filtered_labels.append(label)\n",
    "    return filtered_sequences, filtered_labels\n",
    "\n",
    "X_train_filtered, y_train_filtered = filter_sequences_and_labels(X_train_seq, y_train, top_15_lengths)\n",
    "X_val_filtered, y_val_filtered = filter_sequences_and_labels(X_val_seq, y_val, top_15_lengths)\n",
    "X_test_filtered, y_test_filtered = filter_sequences_and_labels(X_test_seq, y_test, top_15_lengths)\n",
    "\n",
    "y_train = np.array(y_train_filtered)\n",
    "y_val = np.array(y_val_filtered)\n",
    "y_test = np.array(y_test_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yara.mahfouz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - accuracy: 0.9304 - loss: 0.2736 - val_accuracy: 0.9439 - val_loss: 0.1613\n",
      "Epoch 2/15\n",
      "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - accuracy: 0.9634 - loss: 0.1122 - val_accuracy: 0.9579 - val_loss: 0.1327\n",
      "Epoch 3/15\n",
      "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - accuracy: 0.9810 - loss: 0.0606 - val_accuracy: 0.9543 - val_loss: 0.1804\n",
      "Epoch 4/15\n",
      "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 20ms/step - accuracy: 0.9878 - loss: 0.0368 - val_accuracy: 0.9573 - val_loss: 0.1742\n",
      "Epoch 5/15\n",
      "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 20ms/step - accuracy: 0.9927 - loss: 0.0239 - val_accuracy: 0.9564 - val_loss: 0.2031\n",
      "Epoch 6/15\n",
      "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - accuracy: 0.9950 - loss: 0.0174 - val_accuracy: 0.9509 - val_loss: 0.2310\n",
      "Epoch 7/15\n",
      "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - accuracy: 0.9958 - loss: 0.0129 - val_accuracy: 0.9475 - val_loss: 0.3139\n",
      "Epoch 8/15\n",
      "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step - accuracy: 0.9975 - loss: 0.0075 - val_accuracy: 0.9488 - val_loss: 0.3004\n",
      "Epoch 9/15\n",
      "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step - accuracy: 0.9979 - loss: 0.0065 - val_accuracy: 0.9469 - val_loss: 0.3346\n",
      "Epoch 10/15\n",
      "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - accuracy: 0.9988 - loss: 0.0038 - val_accuracy: 0.9490 - val_loss: 0.3398\n",
      "Epoch 11/15\n",
      "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - accuracy: 0.9987 - loss: 0.0038 - val_accuracy: 0.9497 - val_loss: 0.3574\n",
      "Epoch 12/15\n",
      "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - accuracy: 0.9994 - loss: 0.0024 - val_accuracy: 0.9488 - val_loss: 0.3300\n",
      "Epoch 13/15\n",
      "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - accuracy: 0.9989 - loss: 0.0038 - val_accuracy: 0.9494 - val_loss: 0.3419\n",
      "Epoch 14/15\n",
      "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - accuracy: 0.9995 - loss: 0.0021 - val_accuracy: 0.9463 - val_loss: 0.2805\n",
      "Epoch 15/15\n",
      "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - accuracy: 0.9988 - loss: 0.0028 - val_accuracy: 0.9497 - val_loss: 0.4172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x20f819078e0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 25\n",
    "\n",
    "# Pad the sequences to ensure uniform length\n",
    "X_train_padded = pad_sequences(X_train_filtered, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "X_val_padded = pad_sequences(X_val_filtered, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_filtered, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "vocab_size = min(len(tokenizer.word_index) + 1, MAX_NUM_WORDS)\n",
    "\n",
    "def create_lstm_model(input_length, vocab_size, loss_function, lr=0.001):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=128, input_length=input_length),\n",
    "        LSTM(64),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='sigmoid') \n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss=loss_function, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "lstm_model = create_lstm_model(MAX_SEQUENCE_LENGTH, vocab_size, \"binary_crossentropy\")\n",
    "\n",
    "\n",
    "lstm_model.fit(\n",
    "    X_train_padded, y_train,\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_padded, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step\n",
      "Test Macro F1 Score: 0.8295405228018558\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = (lstm_model.predict(X_test_padded) > 0.5).astype(\"int32\")\n",
    "test_macro_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "print(\"Test Macro F1 Score:\", test_macro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model is overfitting, i will fix it using regularization and early stopping to monitor the validation loss and stop training before it overfitts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yara.mahfouz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 33ms/step - accuracy: 0.9254 - loss: 0.2688 - val_accuracy: 0.9520 - val_loss: 0.1564\n",
      "Epoch 2/15\n",
      "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.9705 - loss: 0.0963 - val_accuracy: 0.9566 - val_loss: 0.1713\n",
      "Epoch 3/15\n",
      "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.9832 - loss: 0.0595 - val_accuracy: 0.9568 - val_loss: 0.1691\n",
      "Epoch 4/15\n",
      "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 31ms/step - accuracy: 0.9877 - loss: 0.0417 - val_accuracy: 0.9452 - val_loss: 0.2253\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x20f81d189d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def create_regularized_lstm_model(input_length, vocab_size, loss_function, lr=0.001):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=128, input_length=input_length),\n",
    "        LSTM(64, return_sequences=True),\n",
    "        Dropout(0.4), \n",
    "        LSTM(64),\n",
    "        Dropout(0.4),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss=loss_function, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "regularized_lstm_model = create_regularized_lstm_model(MAX_SEQUENCE_LENGTH, vocab_size, \"binary_crossentropy\")\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "\n",
    "regularized_lstm_model.fit(\n",
    "    X_train_padded, y_train,\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_padded, y_val),\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step\n",
      "Test Macro F1 Score: 0.8307110685864973\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = (regularized_lstm_model.predict(X_test_padded) > 0.5).astype(\"int32\")\n",
    "test_macro_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "print(\"Test Macro F1 Score:\", test_macro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regularization didn't help much but there could be more room for improvements if class imbalance was handled properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed class weights: {0: 0.06867011150394343, 1: 0.9313298884960566}\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yara.mahfouz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 33ms/step - accuracy: 0.7444 - loss: 0.0781 - val_accuracy: 0.8894 - val_loss: 0.0541\n",
      "Epoch 2/15\n",
      "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.9142 - loss: 0.0321 - val_accuracy: 0.8856 - val_loss: 0.0436\n",
      "Epoch 3/15\n",
      "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.9520 - loss: 0.0209 - val_accuracy: 0.9190 - val_loss: 0.0605\n",
      "Epoch 4/15\n",
      "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 33ms/step - accuracy: 0.9725 - loss: 0.0139 - val_accuracy: 0.9253 - val_loss: 0.0515\n",
      "Epoch 5/15\n",
      "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 31ms/step - accuracy: 0.9825 - loss: 0.0089 - val_accuracy: 0.9279 - val_loss: 0.1225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x20f87529150>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
    "total_samples = len(y_train)\n",
    "\n",
    "# Compute weights as the inverse of class frequencies\n",
    "class_weight_dict = {label: total_samples / count for label, count in zip(unique_classes, class_counts)}\n",
    "\n",
    "# Normalize the weights\n",
    "total_weight = sum(class_weight_dict.values())\n",
    "class_weight_dict = {label: weight / total_weight for label, weight in class_weight_dict.items()}\n",
    "\n",
    "print(\"Computed class weights:\", class_weight_dict)\n",
    "\n",
    "weights = [class_weight_dict[0], class_weight_dict[1]]\n",
    "\n",
    "\n",
    "def weighted_binary_crossentropy(weights):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        \n",
    "        # Calculate the binary cross-entropy loss\n",
    "        bce = y_true * tf.math.log(y_pred) * weights[1] + (1 - y_true) * tf.math.log(1 - y_pred) * weights[0]\n",
    "        return -tf.reduce_mean(bce)\n",
    "    return loss\n",
    "\n",
    "weights = [class_weight_dict[0], class_weight_dict[1]]  \n",
    "\n",
    "\n",
    "weighted_lstm_model = create_regularized_lstm_model(MAX_SEQUENCE_LENGTH, vocab_size, weighted_binary_crossentropy(weights))\n",
    "\n",
    "weighted_lstm_model.fit(\n",
    "    X_train_padded, y_train,\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_padded, y_val),\n",
    "    callbacks=[early_stopping]\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "Test Macro F1 Score: 0.7279461417606394\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = (weighted_lstm_model.predict(X_test_padded) > 0.5).astype(\"int32\")\n",
    "test_macro_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "print(\"Test Macro F1 Score:\", test_macro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The results are poor. lets try a more dynamic approach for weighted loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yara.mahfouz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 31ms/step - accuracy: 0.9258 - loss: 0.0078 - val_accuracy: 0.9296 - val_loss: 0.0051\n",
      "Epoch 2/20\n",
      "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.9688 - loss: 0.0033 - val_accuracy: 0.9456 - val_loss: 0.0072\n",
      "Epoch 3/20\n",
      "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.9787 - loss: 0.0021 - val_accuracy: 0.9463 - val_loss: 0.0077\n",
      "Epoch 4/20\n",
      "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.9862 - loss: 0.0012 - val_accuracy: 0.9461 - val_loss: 0.0074\n",
      "Epoch 5/20\n",
      "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.9884 - loss: 0.0010 - val_accuracy: 0.9482 - val_loss: 0.0208\n",
      "Epoch 6/20\n",
      "\u001b[1m690/690\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 31ms/step - accuracy: 0.9913 - loss: 6.6747e-04 - val_accuracy: 0.9507 - val_loss: 0.0152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x20f89a43910>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def focal_loss(gamma=4., alpha=0.75):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        fl = -alpha_t * K.pow(1. - p_t, gamma) * K.log(p_t + K.epsilon())\n",
    "        return K.mean(fl)\n",
    "    return focal_loss_fixed\n",
    "\n",
    "\n",
    "focal_lstm_model = create_regularized_lstm_model(MAX_SEQUENCE_LENGTH, vocab_size, focal_loss(), 0.001)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "\n",
    "focal_lstm_model.fit(\n",
    "    X_train_padded, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_padded, y_val),\n",
    "    callbacks=[early_stopping]\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "Test Macro F1 Score: 0.7966024931983754\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = (focal_lstm_model.predict(X_test_padded) > 0.5).astype(\"int32\")\n",
    "test_macro_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "print(\"Test Macro F1 Score:\", test_macro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The focal loss produced a much better outcome than the tradition class weights approach, but it is still not better than the first basic LSTM model. Let's try resampling techniques instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_padded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yara.mahfouz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 32ms/step - accuracy: 0.6755 - loss: 0.5877 - val_accuracy: 0.8310 - val_loss: 0.3883\n",
      "Epoch 2/15\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step - accuracy: 0.8315 - loss: 0.3855 - val_accuracy: 0.8551 - val_loss: 0.3655\n",
      "Epoch 3/15\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 30ms/step - accuracy: 0.8949 - loss: 0.2576 - val_accuracy: 0.8659 - val_loss: 0.3119\n",
      "Epoch 4/15\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 31ms/step - accuracy: 0.9353 - loss: 0.1641 - val_accuracy: 0.8655 - val_loss: 0.3929\n",
      "Epoch 5/15\n",
      "\u001b[1m1285/1285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 31ms/step - accuracy: 0.9615 - loss: 0.1039 - val_accuracy: 0.8487 - val_loss: 0.6023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x21009ee8700>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smote_lstm_model = create_regularized_lstm_model(MAX_SEQUENCE_LENGTH, vocab_size, \"binary_crossentropy\", 0.001)\n",
    "\n",
    "smote_lstm_model.fit(\n",
    "    X_train_resampled, y_train_resampled,\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_padded, y_val),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "Test Macro F1 Score: 0.6040511542361019\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = (smote_lstm_model.predict(X_test_padded) > 0.5).astype(\"int32\")\n",
    "test_macro_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "print(\"Test Macro F1 Score:\", test_macro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Smote produced the worst results of them all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion and final results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Models\n",
    "- The logistic regression model performed better because its hyperparameters were tuned, svm parameters were not tuned due to time constraints but there is a chance that it might perform slightly better ar at least similar to logistic regression with the right tuning.\n",
    "\n",
    "### Deep Learning Models\n",
    "- The very first deep learning model, which was without regularization or weighted loss performed better than others with such configurations, which was odd given that the model was struggling with class imbalance and overfitting.\n",
    "- when it comes to handling imbalance using weighted loss, focal loss, which uses dynamic weights, was much better than weighted cross entropy loss, although tuning for the gamme and alpha parameters was required (not shown in notebook bu increasing both of these values improved results).\n",
    "- smote performed worst out of all approaches, which signifies that it is generaly not suitable for nlp tasks.\n",
    "\n",
    "### Genral Insights\n",
    "- the machine learning models outperformed the deep learning models, specially logistic regression This likely due to dataset size not being large enough for deep learning model's performance to scale to it. This experimet could highlight the importance of using traditional machine learning models and that deep learning models are not always the best option for any problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nw1GVnYLorxi"
   },
   "source": [
    "#### Done!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
